{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMkh29hihvwlvLceX49VNgU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagodw/RedditBot/blob/master/Reddit_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v0joREaU4hg"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "745mqs9UU9qQ"
      },
      "source": [
        "#! usr/bin/env python3\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
        "from datasets import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOAErpl_VIy_"
      },
      "source": [
        "raw_text = pd.read_csv('/content/drive/MyDrive/reddit_scrape.csv')\n",
        "\n",
        "raw_text.loc[raw_text.parent_text.isna(), 'parent_text'] = raw_text.loc[raw_text.parent_text.isna(), 'title']\n",
        "\n",
        "raw_text['parent_text'] = raw_text.subreddit + '. ' + raw_text.parent_text\n",
        "\n",
        "raw_text['comment_text'] = raw_text.comment_text.str.lower()\n",
        "raw_text['parent_text'] = raw_text.parent_text.str.lower()\n",
        "raw_text = raw_text[(raw_text.comment_text.notna()) & (raw_text.parent_text.notna()) & (raw_text.comment_text != '[deleted]')]\n",
        "\n",
        "raw_text = raw_text[['parent_text', 'comment_text']]\n",
        "\n",
        "fake_data = raw_text[['parent_text']].copy()\n",
        "\n",
        "raw_text['merge_id'] = raw_text.index\n",
        "fake_data['merge_id'] = np.random.randint(0, raw_text.shape[0], size=raw_text.shape[0])\n",
        "fake_data.loc[fake_data.merge_id == fake_data.index, 'merge_id'] = round(fake_data.loc[fake_data.merge_id == fake_data.index, 'merge_id'] / 2 + 10, 0)\n",
        "\n",
        "fake_data = pd.merge(fake_data, raw_text[['merge_id', 'comment_text']], how = 'left', on = 'merge_id')\n",
        "\n",
        "raw_text['label'] = 0\n",
        "fake_data['label'] = 1\n",
        "\n",
        "# raw_text = raw_text.sample(frac = 0.3, replace = False, random_state = 123)\n",
        "# fake_data = fake_data.sample(frac = 0.3, replace = False, random_state = 123)\n",
        "\n",
        "combined_text = raw_text.append(fake_data).drop(['merge_id'], axis = 1).reset_index()\n",
        "combined_text['total_length'] = combined_text.parent_text.str.len() + combined_text.comment_text.str.len()\n",
        "combined_text = combined_text[combined_text.total_length < 1000]\n",
        "combined_text = combined_text.sample(frac=1, random_state = 123)\n",
        "combined_text.head()\n",
        "\n",
        "del raw_text\n",
        "del fake_data\n",
        "\n",
        "print(combined_text.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVVHvDS_VQJm"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOxG1z7SXetW"
      },
      "source": [
        "raw_datasets = Dataset.from_pandas(combined_text[['parent_text', 'comment_text', 'label']])\n",
        "\n",
        "def tokenize_function(examples, max_length = 256):\n",
        "\n",
        "    model_inputs = tokenizer(examples[\"parent_text\"], examples['comment_text'], padding='max_length', truncation=True, max_length=max_length, add_special_tokens=True)\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['parent_text', 'comment_text'])\n",
        "\n",
        "del raw_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldyCJkX63vai"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = 'redditbot_bert',\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size = 32,\n",
        "    warmup_steps = 100,\n",
        "    weight_decay = 0.01,\n",
        "    save_strategy = 'epoch'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msBMIaUqMBIf"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "i = 0\n",
        "\n",
        "prompt1 = combined_text.parent_text[i]\n",
        "prompt1 = prompt1.lower()\n",
        "prompt2 = combined_text.comment_text[i]\n",
        "prompt2 = prompt2.lower()\n",
        "\n",
        "generated = tokenizer(prompt1, prompt2, return_tensors='pt')\n",
        "generated = generated.to(device)\n",
        "\n",
        "model = model.to(device)\n",
        "labs = torch.LongTensor([1])\n",
        "labs = labs.to(device)\n",
        "\n",
        "sample_outputs = model(**generated)[0]\n",
        "\n",
        "lyes = sample_outputs[0][0].item()\n",
        "lno = sample_outputs[0][1].item()\n",
        "\n",
        "pyes = math.exp(lyes) / (math.exp(lyes) + math.exp(lno))\n",
        "print(pyes)\n",
        "\n",
        "print(combined_text.parent_text[i])\n",
        "print(combined_text.comment_text[i])\n",
        "print(combined_text.label[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t2Y8ylbg5Dh"
      },
      "source": [
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "assert logits[0][0] < logits[0][1] # the next sentence was random"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}