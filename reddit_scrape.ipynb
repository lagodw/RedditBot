{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reddit_scrape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhxjMDNsKCXS/MXSIM59Ec",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lagodw/RedditBot/blob/master/reddit_scrape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXGWLQ1vw3E-"
      },
      "source": [
        "%%capture\n",
        "!pip install praw"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUtuoF5Trn-F",
        "outputId": "4f891914-0e6b-41f1-fceb-d398ef6b1be9"
      },
      "source": [
        "#! usr/bin/env python3\n",
        "import praw\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from praw.models import MoreComments\n",
        "import json\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvAd1vA_w2PP"
      },
      "source": [
        "# need to upload file containing reddit account info and secret into json file\n",
        "f = open('reddit_info.json',)\n",
        "reddit_info = json.load(f)\n",
        "reddit = praw.Reddit(client_id=reddit_info['client_id'], \\\n",
        "                     client_secret=reddit_info['client_secret'], \\\n",
        "                     user_agent=reddit_info['user_agent'], \\\n",
        "                     username=reddit_info['username'], \\\n",
        "                     password=reddit_info['password'],\n",
        "                     check_for_async=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcY6bedvoyn0"
      },
      "source": [
        "# list of subreddits with mostly text posts\n",
        "# subreddits = ['IAmA', 'interestingasfuck', 'books', 'tifu', 'news', 'todayilearned', 'damnthatsinteresting',\n",
        "#               'science', 'movies', 'lifeprotips', 'gaming', 'TwoXChromosomes', 'funny', 'explainlikeimfive',\n",
        "#               'mildlyinteresting', 'AskReddit', 'personalfinance', 'Jokes']\n",
        "# alternatively can simply grab the top overall posts\n",
        "subreddits = ['all']\n",
        "append = True\n",
        "write_output = True\n",
        "if 'all' in subreddits:\n",
        "  limit = None\n",
        "else:\n",
        "  limit = 1000\n",
        "\n",
        "if append == True:\n",
        "  output = pd.read_csv('/content/drive/MyDrive/reddit_scrape.csv')\n",
        "else:\n",
        "  output = pd.DataFrame()\n",
        "obs_start = output.shape[0]\n",
        "\n",
        "# pull list of previous threads to make sure we don't duplicate\n",
        "try:\n",
        "  old_id_list = list(output.parent_id.unique())\n",
        "except:\n",
        "  old_id_list = []\n",
        "# print out occasionally to check progress\n",
        "if limit == None:\n",
        "  print_every = 100\n",
        "else:\n",
        "  print_every = limit / 10\n",
        "\n",
        "\n",
        "# structure of model will to predict comments based on the text from the parent comment/post\n",
        "# therefore focus on collecting all comments and their parent text\n",
        "for sub in subreddits:\n",
        "  i = 0\n",
        "  start = dt.datetime.now()\n",
        "  print(f'starting {sub}')\n",
        "\n",
        "  subreddit = reddit.subreddit(sub)\n",
        "  hot_sub = subreddit.hot(limit = limit)\n",
        "\n",
        "  for submission in hot_sub:\n",
        "    if str(submission.id) not in old_id_list:\n",
        "\n",
        "      i += 1\n",
        "      if i % print_every == 0:\n",
        "        print('starting thread {} of subreddit {}'.format(i, sub))\n",
        "\n",
        "      # append to existing id list to ensure we don't duplicate in future\n",
        "      old_id_list += str(submission.id)\n",
        "\n",
        "      comments = pd.DataFrame()\n",
        "      posts = pd.DataFrame()\n",
        "\n",
        "      current_subreddit = str(submission.subreddit)\n",
        "      title = submission.title\n",
        "      post_text = submission.selftext\n",
        "      post_id = submission.id\n",
        "\n",
        "      # limiting factor for speed is Reddit API's 30 query/min where any query of parent text counts as additional query\n",
        "      # to avoid extra requests, instead only record the parent id and match it with other comments \n",
        "      # keep a record of all posts since they will be the parent of any first comments\n",
        "      posts = posts.append(pd.DataFrame({\n",
        "          'comment_text': [post_text],\n",
        "          'comment_id': [str(post_id)]\n",
        "      }), ignore_index = True)\n",
        "\n",
        "      submission.comments.replace_more(limit = 0)\n",
        "\n",
        "      if len(submission.comments.list()) > 0:\n",
        "        for comment in submission.comments.list():\n",
        "\n",
        "          comments = comments.append(pd.DataFrame({\n",
        "              'subreddit': [current_subreddit],\n",
        "              'title': [title],\n",
        "              'parent_id': [str(comment.parent())],\n",
        "              'comment_id': [comment.id],\n",
        "              'comment_text': [comment.body]\n",
        "          }), ignore_index = True)\n",
        "\n",
        "        # create lookup table containing all comments and posts that can then be matched with every comment to obtain parent text\n",
        "        comment_table = comments[['comment_id', 'comment_text']]\n",
        "        lookup_table = posts.append(comment_table)\n",
        "        lookup_table = lookup_table.rename(columns = {'comment_text': 'parent_text', 'comment_id': 'parent_id'})\n",
        "\n",
        "        comments = pd.merge(comments, lookup_table, how = 'left', on = 'parent_id')\n",
        "        output = output.append(comments, ignore_index = True)\n",
        "\n",
        "  if write_output == True:\n",
        "    output.to_csv('/content/drive/MyDrive/reddit_scrape.csv', index = False)\n",
        "  end = dt.datetime.now()\n",
        "  print(f'finished {sub}, took {str(end - start).split(\".\")[0]}')\n",
        "print(f'added {output.shape[0] - obs_start} observations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKjeq6UAxfje"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UZkmtss_wBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}